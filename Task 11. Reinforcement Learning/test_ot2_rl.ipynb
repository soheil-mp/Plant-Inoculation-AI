{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<h1 style=\"text-align:center;\">Test RL in OT2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Initial Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import argparse\n",
    "from collections import deque\n",
    "\n",
    "# Add custom module paths\n",
    "sys.path.append(\"./../task10\")\n",
    "sys.path.append(\"./../task12\")\n",
    "sys.path.append(\"./..\")\n",
    "\n",
    "# Third-party imports\n",
    "import pybullet as p\n",
    "import gymnasium as gym\n",
    "import wandb\n",
    "import torch\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.vec_env import VecNormalize, SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Local imports\n",
    "from callbacks import *\n",
    "from pid_controller import *  \n",
    "from train_ot2_rl import *\n",
    "from ot2_env_wrapper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Setup Env\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configuration\n",
    "hyperparams = {\n",
    "    # Learning parameters\n",
    "    \"learning_rate\": 1e-4,          # Increased for faster learning\n",
    "    \"buffer_size\": 100000,          # Reduced for faster updates\n",
    "    \"batch_size\": 256,              # Keep this\n",
    "    \"tau\": 0.005,                   # Slower target updates for stability\n",
    "    \"gamma\": 0.995,                 # Slightly higher discount for long-term rewards\n",
    "    \"train_freq\": (1, \"step\"),      # Update every step\n",
    "    \"n_envs\": 1,                    # Keep number of environments\n",
    "    \"gradient_steps\": 1,            # Match number of environments\n",
    "    \n",
    "    # Architecture\n",
    "    \"actor_layers\": [512, 512],     # Standard network size\n",
    "    \"critic_layers\": [512, 512],\n",
    "    \"activation_fn\": \"relu\",    \n",
    "    \"net_arch\": \"default\",      \n",
    "    \n",
    "    # Training\n",
    "    \"total_timesteps\": 1_000_000,      # Keep this for now\n",
    "    \"learning_starts\": 1000,         # Increase this to collect more initial data\n",
    "    \"ent_coef\": \"auto_1.0\",        # Automatic entropy adjustment\n",
    "    \n",
    "    # Environment\n",
    "    \"max_episode_steps\": 1_000,      \n",
    "    \"normalize_observations\": True,\n",
    "    \"normalize_rewards\": True,\n",
    "    \"reward_scale\": 1.0,\n",
    "    \"clip_obs\": 5.0,\n",
    "    \"clip_rewards\": 5.0,\n",
    "    \n",
    "    # Evaluation\n",
    "    \"eval_freq\": 50_000,         \n",
    "    \"n_eval_episodes\": 10,     \n",
    "    \"eval_deterministic\": True, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded normalization stats from training\n"
     ]
    }
   ],
   "source": [
    "# Create eval environment with normalization stats\n",
    "eval_env = setup_eval_environment(render=True)\n",
    "\n",
    "# Load normalization statistics from training\n",
    "if os.path.exists(\"./best_model/vec_normalize.pkl\"):\n",
    "    eval_env = VecNormalize.load(\"./best_model/vec_normalize.pkl\", eval_env)\n",
    "    print(\"Loaded normalization stats from training\")\n",
    "else:\n",
    "    print(\"Warning: Could not find normalization stats file\")\n",
    "\n",
    "# Make sure training=False for evaluation\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False  # We don't need reward normalization for evaluation\n",
    "\n",
    "env = eval_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Load Trained Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load(\n",
    "    \"./best_model/best_model.zip\",\n",
    "    env=env,\n",
    "    device='cpu',\n",
    "    custom_objects={\n",
    "        \"learning_rate\": hyperparams[\"learning_rate\"],\n",
    "        \"buffer_size\": hyperparams[\"buffer_size\"],\n",
    "        \"learning_starts\": hyperparams[\"learning_starts\"],\n",
    "        \"batch_size\": hyperparams[\"batch_size\"],\n",
    "        \"tau\": hyperparams[\"tau\"],\n",
    "        \"gamma\": hyperparams[\"gamma\"],\n",
    "        \"train_freq\": hyperparams[\"train_freq\"],\n",
    "        \"gradient_steps\": hyperparams[\"gradient_steps\"],\n",
    "        \"ent_coef\": hyperparams[\"ent_coef\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Test RL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test positions\n",
    "goal_positions = [\n",
    "    # Left position\n",
    "    np.array([-0.0871, 0.0890, 0.1190]),\n",
    "    \n",
    "    # Top-left position\n",
    "    np.array([-0.0571, 0.1590, 0.1190]),\n",
    "    \n",
    "    # Top position\n",
    "    np.array([0.0730, 0.1790, 0.1190]),\n",
    "    \n",
    "    # Top-right position\n",
    "    np.array([0.1830, 0.1590, 0.1190]),\n",
    "    \n",
    "    # Right position\n",
    "    np.array([0.2130, 0.0890, 0.1190]),\n",
    "    \n",
    "    # Bottom-right position\n",
    "    np.array([0.1830, -0.0810, 0.1190]),\n",
    "    \n",
    "    # Bottom position\n",
    "    np.array([0.0730, -0.1105, 0.1190]),\n",
    "    \n",
    "    # Bottom-left position\n",
    "    np.array([-0.0571, -0.0810, 0.1190])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached!\n",
      "Goal reached!\n",
      "Goal reached!\n",
      "Goal reached!\n",
      "Goal reached!\n",
      "Goal reached!\n",
      "Goal reached!\n",
      "Goal reached!\n"
     ]
    }
   ],
   "source": [
    "# Define test positions and action smoothing parameters\n",
    "previous_action = np.zeros(3)  # Initialize previous action\n",
    "time_delay = 0.1  # 100ms between actions\n",
    "\n",
    "for goal_pos in goal_positions:\n",
    "    # Set goal position and reset environment\n",
    "    env.goal_position = goal_pos\n",
    "    obs = env.reset()\n",
    "    previous_action = np.zeros(3)\n",
    "\n",
    "    # Make simulation slower\n",
    "    env.dt = 1/1000\n",
    "    \n",
    "    while True:\n",
    "        # Get action from model and smooth it\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        previous_action = action\n",
    "        \n",
    "        # Take step in environment\n",
    "        obs, reward, terminated, truncated = env.step(action)\n",
    "        time.sleep(time_delay)\n",
    "        \n",
    "        # Check if we reached goal or failed\n",
    "        if terminated:\n",
    "            obs, info = env.reset()\n",
    "            print(\"Goal not reached.\")\n",
    "            break\n",
    "            \n",
    "        # Check if we're close enough to goal\n",
    "        current_pos = obs[:3]\n",
    "        goal_pos = obs[3:6]\n",
    "        distance = np.linalg.norm(goal_pos - current_pos)\n",
    "        if distance < 0.001:\n",
    "            print(\"Goal reached!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
